# Learn to Identify Fundamental Sub-Tasks from One-Shot Video Demonstration of a Complete Task for Robot Execution #

**Introduction**
Robotic systems can learn tasks from human demonstrations using videos captured by vision sensors, which could significantly improve applications involving complex tasks. Traditional methods struggle with intricate patterns and long-horizon tasks, but sub-task classification is crucial for fields like computer vision, autonomous systems, and human-robot interaction. In healthcare, human-robot collaboration is transformative, though programming robots for complex tasks remains challenging. We propose a hybrid variance-based DNN (Hybrid V-DNN) using transfer learning with a modified VGG-16 network to address this. Our model captures pixel variance and hand movements to segment human actions into sub-tasks like reach and retract. This approach improves accuracy, reduces computational costs, and enables real-time processing for dynamic tasks.

## Preprocessing steps followed by our approach ##
![Preprosessing_flowchart](https://github.com/user-attachments/assets/f0e9d423-bab7-498a-a0c8-d448a4567372)

## Workflow for hybrid sub-task identification approach from any video demonstration. (HM represents hand movement.) ##
![Screenshot 2024-09-19 135630](https://github.com/user-attachments/assets/073c79a2-3160-452b-ae6b-d64a0fa7cc43)


  _Experimental Results_
