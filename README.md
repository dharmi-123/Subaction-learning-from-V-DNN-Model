# Learn to Identify Fundamental Sub-Tasks from One-Shot Video Demonstration of a Complete Task for Robot Execution #

**Introduction**
Robotic systems can learn tasks from human demonstrations using videos captured by vision sensors, which could significantly improve applications involving complex tasks. Traditional methods struggle with intricate patterns and long-horizon tasks, but sub-task classification is crucial for fields like computer vision, autonomous systems, and human-robot interaction. In healthcare, human-robot collaboration is transformative, though programming robots for complex tasks remains challenging. We propose a hybrid variance-based DNN (Hybrid V-DNN) using transfer learning with a modified VGG-16 network to address this. Our model captures pixel variance and hand movements to segment human actions into sub-tasks like reach and retract. This approach improves accuracy, reduces computational costs, and enables real-time processing for dynamic tasks.

  _Experimental Results_
